# Neural Network from Scratch

A comprehensive implementation of a 2-layer Neural Network for MNIST digit classification built entirely from scratch using NumPy, along with an equivalent PyTorch implementation for comparison.

## üéØ Project Overview

This project demonstrates the fundamental concepts of deep learning by implementing a neural network without relying on high-level frameworks like TensorFlow or PyTorch. The from-scratch implementation provides deep insights into the mathematical foundations and computational mechanics that power modern deep learning frameworks.

## üöÄ Why Build from Scratch?

While powerful frameworks like TensorFlow and PyTorch exist, building neural networks from scratch offers several educational advantages:

- **Mathematical Understanding**: Gain deep insights into backpropagation, gradient descent, and optimization algorithms
- **Implementation Clarity**: Understand exactly what happens behind the scenes in high-level frameworks
- **Debugging Skills**: Learn to identify and fix issues at the algorithmic level
- **Foundation Building**: Establish a solid base for understanding advanced architectures
- **Performance Insights**: Appreciate the optimizations that frameworks provide

## üèóÔ∏è Architecture

### Network Structure
```
Input Layer (784 neurons) ‚Üí Hidden Layer (128 neurons) ‚Üí Output Layer (10 neurons)
        ‚Üì                           ‚Üì                         ‚Üì
   28√ó28 pixels              ReLU Activation           Softmax Activation
```

### Mathematical Formulation

#### Forward Propagation
1. **Hidden Layer:**
   ```
   Z‚ÇÅ = W‚ÇÅ * X + b‚ÇÅ
   A‚ÇÅ = ReLU(Z‚ÇÅ) = max(0, Z‚ÇÅ)
   ```

2. **Output Layer:**
   ```
   Z‚ÇÇ = W‚ÇÇ * A‚ÇÅ + b‚ÇÇ
   A‚ÇÇ = Softmax(Z‚ÇÇ) = exp(Z‚ÇÇ·µ¢ - max(Z‚ÇÇ)) / Œ£‚±º exp(Z‚ÇÇ‚±º - max(Z‚ÇÇ))
   ```

#### Loss Function
**Cross-Entropy Loss:**
```
Loss = -1/m * Œ£·µ¢ Œ£‚±º y·µ¢‚±º * log(≈∑·µ¢‚±º + Œµ)
```
where:
- m = number of samples
- y·µ¢‚±º = true label (one-hot encoded)
- ≈∑·µ¢‚±º = predicted probability
- Œµ = small constant (1e-8) to prevent log(0)

#### Backward Propagation
1. **Output Layer Gradients:**
   ```
   dZ‚ÇÇ = A‚ÇÇ - Y
   dW‚ÇÇ = 1/m * dZ‚ÇÇ * A‚ÇÅ·µÄ
   db‚ÇÇ = 1/m * Œ£(dZ‚ÇÇ)
   ```

2. **Hidden Layer Gradients:**
   ```
   dA‚ÇÅ = W‚ÇÇ·µÄ * dZ‚ÇÇ
   dZ‚ÇÅ = dA‚ÇÅ * ReLU'(Z‚ÇÅ)
   dW‚ÇÅ = 1/m * dZ‚ÇÅ ¬∑ X·µÄ
   db‚ÇÅ = 1/m * Œ£(dZ‚ÇÅ)
   ```

#### Parameter Updates
**Gradient Descent:**
```
W‚ÇÅ := W‚ÇÅ - Œ± * dW‚ÇÅ
b‚ÇÅ := b‚ÇÅ - Œ± * db‚ÇÅ
W‚ÇÇ := W‚ÇÇ - Œ± * dW‚ÇÇ
b‚ÇÇ := b‚ÇÇ - Œ± * db‚ÇÇ
```
where Œ± is the learning rate.

#### Weight Initialization
**He Initialization (for ReLU):**
```
W ~ N(0, ‚àö(2/n_in))
```
where n_in is the number of input neurons to the layer.

## üìä Implementation Details

### Data Preprocessing
- **Normalization**: Pixel values scaled from [0, 255] to [0, 1]
- **One-Hot Encoding**: Labels converted to 10-dimensional vectors
- **Train-Test Split**: 80% training, 20% testing

### Hyperparameters
- **Learning Rate**: 0.001
- **Epochs**: 30
- **Hidden Layer Size**: 128 neurons
- **Batch Processing**: Full dataset (can be modified for mini-batch)

## üîß Usage

### Running the From-Scratch Implementation
```bash
# Open the Jupyter notebook
jupyter notebook neural_network_scratch.ipynb

# Or run directly if converted to Python script
python neural_network_scratch.py
```



## üîç Key Differences: From-Scratch vs PyTorch

| Aspect | From-Scratch (NumPy) | PyTorch |
|--------|----------------------|---------|
| **Weight Initialization** | Manual He initialization | `torch.nn.init.kaiming_normal_()` |
| **Forward Pass** | Manual matrix operations | `nn.Linear()` + `F.relu()` |
| **Loss Calculation** | Manual cross-entropy | `nn.CrossEntropyLoss()` |
| **Backward Pass** | Manual gradient calculation | `loss.backward()` |
| **Parameter Updates** | Manual gradient descent | `optimizer.step()` |
| **Data Loading** | Manual preprocessing | `DataLoader` with transforms |
| **Code Length** | Lengthy | Shorter |
| **Learning Value** | High (understand internals) | Medium (framework usage) |

## üìà Results

Both implementations achieve similar performance on MNIST:
- **Accuracy**: ~95-98% on test set
- **Training Time**: From-scratch is slower but educational
- **Memory Usage**: PyTorch is more optimized

## üéì Learning Outcomes

After completing this project, you will understand:

1. **Mathematical Foundations**: The math behind neural networks
2. **Implementation Details**: How frameworks work internally
3. **Optimization Techniques**: Why certain choices are made
4. **Debugging Skills**: How to identify and fix algorithmic issues
5. **Framework Appreciation**: Why PyTorch/TensorFlow are powerful


## üìù License

This project is open source and available under the [MIT License](LICENSE).

