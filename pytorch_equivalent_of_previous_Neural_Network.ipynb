{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db8eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the same Neural Network which was implemented from Scratch in the other notebook but here we will be using Pytorch to implement it.\n",
    "import torch # Main torch library\n",
    "import torch.nn as nn  # For building neural network layers like Linear, Conv2d etc.\n",
    "import torch.nn.functional as F # For using activation functions like ReLU, SoftMax etc.\n",
    "import torch.optim as optim   # For optimizers like Adam\n",
    "from torchvision import datasets, transforms # For preprocessing datasets and applying transformations\n",
    "from torch.utils.data import DataLoader # For loading datasets in batches\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1bc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet is the equivalent of the X = X/ 255.0 the resizing part, the train_test_split() part and the X_train.T, Y_train.T in batches\n",
    "transforms = transforms.ToTensor() \n",
    "train_dataset = datasets.MNIST(root = \"./data\", train= True, transform= transforms, download=True)\n",
    "test_dataset = datasets.MNIST(root = \"./data\", train=False, transform= transforms, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af41c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# The shape is of the format [Batch Size, Channels, Height, Width]\n",
    "# While building from scratch it was [70000, 784] currently it is [64, 1, 28, 28] \n",
    "data = iter(train_loader)\n",
    "images, labels = next(data)\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb73b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No manual One Hot Encoding is needed here as Pytorch's CrossEntropyLoss function automatically handles it. It directly takes the class indices as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7582d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No manual train-test-split is needed as torch MNIST dataset has already been split into train and test datasets accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b072149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nanda\\AppData\\Local\\Temp\\ipykernel_19976\\1387640033.py:13: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  init.kaiming_normal(self.fc1.weight, nonlinearity='relu') # He Initialization for W1\n",
      "C:\\Users\\nanda\\AppData\\Local\\Temp\\ipykernel_19976\\1387640033.py:15: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  init.kaiming_normal(self.fc2.weight, nonlinearity='relu') # He Initialization for W2\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "\n",
    "class WeightInitialization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightInitialization, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Explicitly define the Input Layer (input_size, hidden_size) shape kind of W1 & b1 \n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Explicitly define the Hidden Layer (hidden_size, output_size) shape kind of W2 & b2\n",
    "\n",
    "        init.kaiming_normal(self.fc1.weight, nonlinearity='relu') # He Initialization for W1\n",
    "        init.zeros_(self.fc1.bias) # Initialize b1 to 0\n",
    "        init.kaiming_normal(self.fc2.weight, nonlinearity='relu') # He Initialization for W2\n",
    "        init.zeros_(self.fc2.bias) # Initiialize b2 to 0\n",
    "\n",
    "model = WeightInitialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657df3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight.shape)  # (128, 784)\n",
    "print(model.fc1.bias.shape)    # (128)\n",
    "print(model.fc2.weight.shape)  # (10, 128)\n",
    "print(model.fc2.bias.shape)    # (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9215233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch has already built in functions for activation functions such as ReLU, Softmax etc so we dont need to define them explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "addce4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Forward progpagation in Pytorch is done as follows:\n",
    "import torch.nn.functional as F\n",
    "class ForwardPropagation(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(ForwardPropagation, self).__init__()\n",
    "        self.fc1 = layers.fc1\n",
    "        self.fc2 = layers.fc2\n",
    "\n",
    "    def forward(self, x):\n",
    "            Z1 = self.fc1(x)\n",
    "            A1 = F.relu(Z1)\n",
    "            Z2 = self.fc2(A1)\n",
    "            # IMPORTANT : Dont Apply SoftMax Activation Here as CrossEntropy Loss expects raw logits not softmaxed outputs\n",
    "            return Z2\n",
    "        \n",
    "\n",
    "forwardpropagation = ForwardPropagation(model)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1740a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3792948722839355\n"
     ]
    }
   ],
   "source": [
    "#All these have to be incorporated in the main training loop\n",
    "from torch.optim import Adam\n",
    "data = iter(train_loader)\n",
    "images, labels = next(data) # images shape - [64, 1, 28, 28] labels shape - [64]\n",
    "images = images.view(images.size(0), -1)  # Shape converted to [64, 784] for fully connected layers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # The Main role comes in Backprop but here we clear older gradients for optimizing memory footprint.\n",
    "optimizer.zero_grad()\n",
    "outputs = forwardpropagation(images)\n",
    "criterion = nn.CrossEntropyLoss()   \n",
    "loss = criterion(outputs, labels) \n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BackPropgation is just one line in Pytorch thats it All these have to be incorporated in the main training loop\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80921314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The updation of parameters is also a single liner in pytorch That's it. All these have to be incorporated in the main training loop\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a3d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 0.3064, Accuracy: 96.75%\n",
      "Epoch 2/30 - Loss: 0.2933, Accuracy: 96.91%\n",
      "Epoch 3/30 - Loss: 0.2813, Accuracy: 97.07%\n",
      "Epoch 4/30 - Loss: 0.2862, Accuracy: 97.16%\n",
      "Epoch 5/30 - Loss: 0.2827, Accuracy: 97.12%\n",
      "Epoch 6/30 - Loss: 0.2752, Accuracy: 97.29%\n",
      "Epoch 7/30 - Loss: 0.2763, Accuracy: 97.32%\n",
      "Epoch 8/30 - Loss: 0.2828, Accuracy: 97.33%\n",
      "Epoch 9/30 - Loss: 0.2695, Accuracy: 97.41%\n",
      "Epoch 10/30 - Loss: 0.2742, Accuracy: 97.46%\n",
      "Epoch 11/30 - Loss: 0.2714, Accuracy: 97.43%\n",
      "Epoch 12/30 - Loss: 0.2769, Accuracy: 97.45%\n",
      "Epoch 13/30 - Loss: 0.2759, Accuracy: 97.48%\n",
      "Epoch 14/30 - Loss: 0.2806, Accuracy: 97.47%\n",
      "Epoch 15/30 - Loss: 0.2755, Accuracy: 97.49%\n",
      "Epoch 16/30 - Loss: 0.2870, Accuracy: 97.44%\n",
      "Epoch 17/30 - Loss: 0.2935, Accuracy: 97.43%\n",
      "Epoch 18/30 - Loss: 0.2893, Accuracy: 97.48%\n",
      "Epoch 19/30 - Loss: 0.2957, Accuracy: 97.48%\n",
      "Epoch 20/30 - Loss: 0.3012, Accuracy: 97.49%\n",
      "Epoch 21/30 - Loss: 0.2983, Accuracy: 97.50%\n",
      "Epoch 22/30 - Loss: 0.3034, Accuracy: 97.50%\n",
      "Epoch 23/30 - Loss: 0.3071, Accuracy: 97.55%\n",
      "Epoch 24/30 - Loss: 0.3080, Accuracy: 97.51%\n",
      "Epoch 25/30 - Loss: 0.3087, Accuracy: 97.52%\n",
      "Epoch 26/30 - Loss: 0.3202, Accuracy: 97.49%\n",
      "Epoch 27/30 - Loss: 0.3112, Accuracy: 97.51%\n",
      "Epoch 28/30 - Loss: 0.3149, Accuracy: 97.52%\n",
      "Epoch 29/30 - Loss: 0.3264, Accuracy: 97.48%\n",
      "Epoch 30/30 - Loss: 0.3200, Accuracy: 97.54%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "losses = []\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    _, y_pred_labels = torch.max(y_pred, 1)\n",
    "    return (y_pred_labels == y_true).float().mean() * 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    forwardpropagation.train()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        # Forward propagation\n",
    "        outputs = forwardpropagation(images)\n",
    "\n",
    "        #Cross Entropy Loss\n",
    "        criterion = nn.CrossEntropyLoss()   \n",
    "        loss = criterion(outputs, labels) \n",
    "\n",
    "        #Backward propagation\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy(labels, outputs)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_accuracy = epoch_accuracy / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.2f}%\")\n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
